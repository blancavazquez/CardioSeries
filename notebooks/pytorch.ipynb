{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd307915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blanca/anaconda3/envs/py38/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "def weight_init(m):\n",
    "    #sustituye a :kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05,seed=42)\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "      torch.nn.init.trunc_normal_(m.weight, 0.0, 0.05)\n",
    "\n",
    "class CausalConv1d(torch.nn.Conv1d):\n",
    "    def __init__(self,num_conditions,num_features,input_seq_length,in_channels,out_channels,\n",
    "                 kernel_size,stride=1,dilation=1,groups=1,bias=True):\n",
    "        \n",
    "        self.__padding = (kernel_size - 1) * dilation\n",
    "        \n",
    "        super(CausalConv1d, self).__init__(\n",
    "            in_channels,in_channels,kernel_size=kernel_size,stride=stride,\n",
    "            padding=self.__padding,dilation=dilation,groups=groups,bias=bias)\n",
    "\n",
    "        self.hidden_layer0 = nn.Sequential(\n",
    "            nn.Linear(input_seq_length*num_features+num_conditions, input_seq_length*num_features))\n",
    "\n",
    "        self.cnn_block_1 = nn.Conv1d(\n",
    "            in_channels=input_seq_length,out_channels=input_seq_length,kernel_size=1,bias=False)\n",
    "\n",
    "    \n",
    "    def forward(self, x_input,cond):\n",
    "        \"\"\"\n",
    "        x_input = [batch_size,input_setlength,num_features]\n",
    "        cond = [batch_size,num_conditions]\n",
    "        \"\"\"\n",
    "        x_input_np = x_input.reshape(-1,x_input.shape[1]*x_input.shape[2])\n",
    "        cond = cond.long()\n",
    "        x = torch.cat([x_input_np,cond],1) #[N,input_setlength*num_features+num_conditions]\n",
    "        x = self.hidden_layer0(x) #[32,42]\n",
    "        x = x.reshape(-1,x_input.shape[1],x_input.shape[2]) #[N,input_setlength,num_features]\n",
    "          \n",
    "        #------- Encoder -----------#\n",
    "        residual = x #[16,6,7]\n",
    "        layer_out = super(CausalConv1d, self).forward(x)\n",
    "        layer_out = layer_out[:, :, :-self.__padding] if self.__padding != 0 else layer_out\n",
    "        layer_out = F.selu(layer_out) #[32,6,7]\n",
    "          \n",
    "        #------- Decoder -----------#\n",
    "        skip_out = self.cnn_block_1(layer_out)\n",
    "        network_in = self.cnn_block_1(layer_out)\n",
    "        network_out = residual + network_in #[32,6,7]\n",
    "        return network_out, skip_out\n",
    "\n",
    "class CausalModel(pl.LightningModule):\n",
    "    def __init__(self, w_decay,dropout,alpha,gamma,input_seq_length,output_seq_length,\n",
    "                 num_features,lr,num_conditions, path,feature_list,\n",
    "                 net,in_channels,out_channels,kernel_size,stride,dilation,groups,bias):\n",
    "\n",
    "        super(CausalModel,self).__init__()\n",
    "        self.CausalConv1d = CausalConv1d(num_conditions,num_features,input_seq_length,in_channels,out_channels,kernel_size,stride,dilation,groups,bias)\n",
    "        self.w_decay = w_decay\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.input_seq_length = input_seq_length\n",
    "        self.output_seq_length = output_seq_length\n",
    "        self.num_features = num_features\n",
    "        self.lr = lr\n",
    "        self.num_conditions = num_conditions\n",
    "        self.path = path\n",
    "        self.feature_list = feature_list\n",
    "        self.net = net\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize weights\n",
    "        self.CausalConv1d.apply(weight_init)\n",
    "\n",
    "    def forward(self, x_input,cond,output_seq_length): #[32,6,7]\n",
    "        dev = x_input.device\n",
    "        l1a, l1b = CausalConv1d(num_conditions=cond.shape[1],num_features=x_input.shape[2],input_seq_length=x_input.shape[1],in_channels=self.input_seq_length, out_channels=self.input_seq_length, kernel_size=2, dilation=1).to(dev)(x_input,cond)\n",
    "        l2a, l2b = CausalConv1d(num_conditions=cond.shape[1],num_features=l1a.shape[2],input_seq_length=l1a.shape[1],in_channels=l1a.shape[1], out_channels=output_seq_length, kernel_size=2, dilation=2).to(dev)(l1a,cond)\n",
    "        l3a, l3b = CausalConv1d(num_conditions=cond.shape[1],num_features=l2a.shape[2],input_seq_length=l2a.shape[1],in_channels=l2a.shape[1], out_channels=output_seq_length, kernel_size=2, dilation=4).to(dev)(l2a,cond)\n",
    "        l4a, l4b = CausalConv1d(num_conditions=cond.shape[1],num_features=l3a.shape[2],input_seq_length=l3a.shape[1],in_channels=l3a.shape[1], out_channels=output_seq_length, kernel_size=2, dilation=8).to(dev)(l3a,cond)\n",
    "        l5a, l5b = CausalConv1d(num_conditions=cond.shape[1],num_features=l4a.shape[2],input_seq_length=l4a.shape[1],in_channels=l4a.shape[1], out_channels=output_seq_length, kernel_size=2, dilation=16).to(dev)(l4a,cond)\n",
    "        l6a, l6b = CausalConv1d(num_conditions=cond.shape[1],num_features=l5a.shape[2],input_seq_length=l5a.shape[1],in_channels=l5a.shape[1], out_channels=output_seq_length, kernel_size=2, dilation=32).to(dev)(l5a,cond)\n",
    "        l6b = nn.Dropout(p=0.8)(l6b) #dropout used to limit influence of earlier data\n",
    "        l7a, l7b = CausalConv1d(num_conditions=cond.shape[1],num_features=l6a.shape[2],input_seq_length=l6a.shape[1],in_channels=l6a.shape[1], out_channels=output_seq_length, kernel_size=2, dilation=64).to(dev)(l6a,cond)\n",
    "        l7b = nn.Dropout(p=0.8)(l7b) #dropout used to limit influence of earlier data\n",
    "\n",
    "        l8 =  l1b + l2b + l3b + l4b + l5b + l6b + l7b\n",
    "\n",
    "        l9 = F.leaky_relu(l8)\n",
    "        l21 = nn.Conv1d(in_channels=l9.shape[1],out_channels=output_seq_length,kernel_size=1,bias=False).to(dev)(l9)#[32, 6, 7]\n",
    "        return l21\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        target_in, target_out,condition, mask = batch\n",
    "        condition = condition[:,:self.num_conditions] #settings to number of conditions\n",
    "\n",
    "        target_in = torch.tensor(target_in, dtype=torch.float32).to(target_in.device)#[32, 6, 7]\n",
    "        target_out = torch.tensor(target_out, dtype=torch.float32).to(target_out.device)#[32, 6, 7]        \n",
    "        y_pred = self(target_in,condition,target_out.shape[1])#[32, 6, 7]\n",
    "\n",
    "        synth_mask = torch.masked_select(y_pred, mask)\n",
    "        real_mask = torch.masked_select(target_out, mask)\n",
    "\n",
    "        rmse = rmse_loss(synth_mask,real_mask)\n",
    "        self.log(\"loss_train\", rmse,on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\":rmse,\n",
    "                \"past\":target_in,\"ytrue\":target_out,\"ypred\":y_pred, \n",
    "                \"conditions\":condition, \"mask\":mask}\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        loss = torch.flatten(torch.stack([x['loss'] for x in training_step_outputs]))\n",
    "        loss = loss.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        saving_logs_training(loss)\n",
    "\n",
    "        # Convert from list to tensor\n",
    "        past = torch.flatten(torch.stack([x['past'] for x in training_step_outputs]))\n",
    "        ytrue = torch.flatten(torch.stack([x['ytrue'] for x in training_step_outputs]))\n",
    "        ypred = torch.flatten(torch.stack([x['ypred'] for x in training_step_outputs]))\n",
    "        conditions = torch.flatten(torch.stack([x['conditions'] for x in training_step_outputs]))\n",
    "        mask = torch.flatten(torch.stack([x['mask'] for x in training_step_outputs]))\n",
    "\n",
    "        past = past.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        ytrue = ytrue.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        ypred = ypred.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        conditions = conditions.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        mask = mask.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        target_in, target_out,condition, mask = batch\n",
    "        condition = condition[:,:self.num_conditions] #settings to number of conditions\n",
    "\n",
    "        target_in = torch.tensor(target_in, dtype=torch.float32).to(target_in.device)#[32, 24, 8]\n",
    "        target_out = torch.tensor(target_out, dtype=torch.float32).to(target_out.device)#[32, 24, 8]        \n",
    "        y_pred = self(target_in,condition,target_out.shape[1])#[32, 24, 8]    \n",
    "        \n",
    "        #------- Computing RMSE loss (using masking)-------#\n",
    "        synth_mask = torch.masked_select(y_pred, mask)\n",
    "        real_mask = torch.masked_select(target_out, mask)\n",
    "\n",
    "        rmse = rmse_loss(synth_mask,real_mask)\n",
    "\n",
    "        self.log(\"loss_val\",rmse)\n",
    "        return {\"loss\":rmse,\n",
    "                \"past\":target_in,\"ytrue\":target_out,\"ypred\":y_pred, \n",
    "                \"conditions\":condition, \"mask\":mask}\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        loss_val = torch.flatten(torch.stack([x['loss'] for x in validation_step_outputs]))\n",
    "        loss_val = loss_val.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        saving_logs_validation(loss_val)\n",
    "\n",
    "        # Convert from list to tensor\n",
    "        past_val = torch.flatten(torch.stack([x['past'] for x in validation_step_outputs]))\n",
    "        ytrue_val = torch.flatten(torch.stack([x['ytrue'] for x in validation_step_outputs]))\n",
    "        ypred_val = torch.flatten(torch.stack([x['ypred'] for x in validation_step_outputs]))\n",
    "        conditions_val = torch.flatten(torch.stack([x['conditions'] for x in validation_step_outputs]))\n",
    "        mask_val = torch.flatten(torch.stack([x['mask'] for x in validation_step_outputs]))\n",
    "\n",
    "        past_val = past_val.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        ytrue_val = ytrue_val.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        ypred_val = ypred_val.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        conditions_val = conditions_val.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "        mask_val = mask_val.view(-1).detach().cpu().numpy().reshape(-1,1)\n",
    "\n",
    "        plotting_predictions(past_val,ytrue_val,ypred_val,mask_val,self.input_seq_length,self.output_seq_length,\n",
    "                             self.num_features,self.num_conditions,conditions_val,self.path,\"dcnn\",1,\"val\",self.current_epoch)\n",
    "        dwprobability(ytrue_val,ypred_val,self.output_seq_length,self.num_features,self.path,self.net,self.current_epoch,\"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #weight_decay sustituye a: kernel_regularizer=l2(l2_layer_reg))\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=0.00075,betas=(0.9, 0.999),weight_decay=0.001)        \n",
    "        return {\"optimizer\": optimizer,\"monitor\": \"loss\",}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6609058f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0852, 0.3519, 0.0379, 0.4666, 0.5846, 0.8126, 0.3698],\n",
      "         [0.7432, 0.1818, 0.4830, 0.7041, 0.3490, 0.7891, 0.1373],\n",
      "         [0.5632, 0.2166, 0.4447, 0.5470, 0.3433, 0.3877, 0.2046],\n",
      "         [0.7716, 0.6005, 0.5688, 0.4146, 0.4406, 0.4069, 0.4131],\n",
      "         [0.2992, 0.6376, 0.8898, 0.7747, 0.6553, 0.1490, 0.0798],\n",
      "         [0.3513, 0.0342, 0.7974, 0.9686, 0.3627, 0.5371, 0.7010]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand([1,6,7]) #[batch_size,input_seqlength,num_features]\n",
    "print(tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
